<!DOCTYPE html>
<html>
<head>
  <title> Programs</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background: #f5f5f5;
      padding: 20px;
    }
    h1 {
      text-align: center;
    }
    .program {
      background: #fff;
      padding: 15px;
      margin: 20px auto;
      border-radius: 8px;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
      width: 90%;
      max-width: 900px;
    }
    .program h2 {
      margin-top: 0;
      color: #333;
    }
    pre {
      background: #eee;
      padding: 10px;
      overflow-x: auto;
      border-radius: 5px;
    }
  </style>
</head>
<body>



  <div class="program">
    <h2>1. Implementation of the McCulloch Pitts Neuron Model for the Basic Logic Gates</h2>
    <pre>
 import numpy as np

def mcp_neuron(inputs, weights, threshold):
    weighted_sum = np.dot(inputs, weights)
    return 1 if weighted_sum >= threshold else 0

def logic_gates():
    print("McCulloch Pitts Neuron for Logic Gates")
    
    inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    
    print("AND Gate")
    and_weights = np.array([1, 1])
    and_threshold = 2
    for x in inputs:
        output = mcp_neuron(x, and_weights, and_threshold)
        print(f"Input: {x}, Output: {output}")
    
    print("\nOR Gate")
    or_weights = np.array([1, 1])
    or_threshold = 1
    for x in inputs:
        output = mcp_neuron(x, or_weights, or_threshold)
        print(f"Input: {x}, Output: {output}")
    
    print("\nNOT Gate")
    not_inputs = np.array([0, 1])
    not_weights = np.array([-1])
    not_threshold = 0
    for x in not_inputs:
        output = mcp_neuron(np.array([x]), not_weights, not_threshold)
        print(f"Input: {x}, Output: {output}")
    
    print("\nXOR Gate")
    for x in inputs:
        s1 = mcp_neuron(x, np.array([1, 1]), 1)
        s2 = mcp_neuron(x, np.array([1, 1]), 2)
        intermediate = np.array([s1, 1-s2])
        xor_weights = np.array([1, 1])
        output = mcp_neuron(intermediate, xor_weights, 2)
        print(f"Input: {x}, Output: {output}")

logic_gates()
    </pre>
  </div>

  <div class="program">
    <h2>2. Implementation of Perceptron Learning Algorithm for Binary Classification</h2>
    <pre>
import numpy as np

X = np.array([[2, 3], [1, 1], [4, 5], [5, 4], [1, 4], [3, 2]])
y = np.array([1, 1, 1, 0, 0, 0])

weights = np.zeros(X.shape[1])
bias = 0
learning_rate = 0.1
epochs = 10

for epoch in range(epochs):
    for i in range(X.shape[0]):
        prediction = 1 if (np.dot(X[i], weights) + bias) >= 0 else 0
        error = y[i] - prediction
        weights += learning_rate * error * X[i]
        bias += learning_rate * error

print("Final weights:", weights)
print("Final bias:", bias)

print("Predictions:")
for i in range(X.shape[0]):
    prediction = 1 if (np.dot(X[i], weights) + bias) >= 0 else 0
    print(f"Input: {X[i]}, Predicted: {prediction}, Actual: {y[i]}")
    </pre>
  </div>

  <div class="program">
    <h2>3. Implementation of Different Activation Functions to Train a Simple Neural Network</h2>
    <pre>
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - x**2

X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

np.random.seed(1)
weights_input_hidden = np.random.uniform(size=(2, 2))
weights_hidden_output = np.random.uniform(size=(2, 1))
bias_hidden = np.zeros((1, 2))
bias_output = np.zeros((1, 1))

learning_rate = 0.1
epochs = 1000

activation_functions = [("Sigmoid", sigmoid, sigmoid_derivative),
                       ("ReLU", relu, relu_derivative),
                       ("Tanh", tanh, tanh_derivative)]

for name, activation, derivative in activation_functions:
    print(f"\nTraining with {name} activation function")
    
    w_ih = weights_input_hidden.copy()
    w_ho = weights_hidden_output.copy()
    b_h = bias_hidden.copy()
    b_o = bias_output.copy()
    
    for epoch in range(epochs):
        hidden_input = np.dot(X, w_ih) + b_h
        if name == "Tanh":
            hidden_output = activation(hidden_input)
        else:
            hidden_output = activation(hidden_input)
        
        output_input = np.dot(hidden_output, w_ho) + b_o
        output = sigmoid(output_input)
        
        error = y - output
        output_delta = error * sigmoid_derivative(output)
        
        hidden_error = np.dot(output_delta, w_ho.T)
        hidden_delta = hidden_error * derivative(hidden_output)
        
        w_ho += learning_rate * np.dot(hidden_output.T, output_delta)
        w_ih += learning_rate * np.dot(X.T, hidden_delta)
        b_o += learning_rate * np.sum(output_delta, axis=0, keepdims=True)
        b_h += learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)
    
    print("Predictions:")
    hidden_input = np.dot(X, w_ih) + b_h
    hidden_output = activation(hidden_input) if name != "Tanh" else activation(hidden_input)
    output = sigmoid(np.dot(hidden_output, w_ho) + b_o)
    for i in range(len(X)):
        print(f"Input: {X[i]}, Predicted: {output[i][0]:.4f}, Actual: {y[i][0]}")
    </pre>
  </div>

  <div class="program">
    <h2>4. Implementation of MLP (Multilayer Perceptron) for Binary Classification (Spam Detection)</h2>
    <pre>
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
import numpy as np

X = np.array([[0, 1, 2], [1, 2, 3], [2, 0, 1], [3, 1, 0], [1, 1, 1], [0, 0, 0]])
y = np.array([0, 1, 1, 0, 1, 0])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

mlp = MLPClassifier(hidden_layer_sizes=(5,), activation='relu', max_iter=1000, random_state=1)
mlp.fit(X_scaled, y)

predictions = mlp.predict(X_scaled)

print("MLP for Spam Detection")
print("Features (scaled):", X_scaled)
print("Actual labels:", y)
print("Predicted labels:", predictions)

accuracy = np.mean(predictions == y)
print(f"Accuracy: {accuracy:.4f}")
    </pre>
  </div>

  <div class="program">
    <h2>5. Implementation of Feedforward Networks for Handwritten Digit Recognition</h2>
    <pre>
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train = x_train.reshape((60000, 28*28)).astype('float32') / 255
x_test = x_test.reshape((10000, 28*28)).astype('float32') / 255

y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

model = models.Sequential()
model.add(layers.Dense(128, activation='relu', input_shape=(28*28,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {test_accuracy:.4f}")

predictions = model.predict(x_test[:5])
print("Predictions for first 5 test images:")
for i in range(5):
    predicted_label = np.argmax(predictions[i])
    actual_label = np.argmax(y_test[i])
    print(f"Sample {i+1}: Predicted: {predicted_label}, Actual: {actual_label}")
    </pre>
  </div>

  <div class="program">
    <h2>6. Implement a CNN to Classify Faces as Male or Female Based on Facial Features</h2>
    <pre>
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

np.random.seed(1)
x_train = np.random.rand(100, 64, 64, 1)
y_train = np.random.randint(0, 2, (100,))
x_test = np.random.rand(20, 64, 64, 1)
y_test = np.random.randint(0, 2, (20,))

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=3, batch_size=16, verbose=1)

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {test_accuracy:.4f}")

predictions = model.predict(x_test[:5])
print("Predictions for first 5 test images:")
for i in range(5):
    predicted_label = 1 if predictions[i] >= 0.5 else 0
    print(f"Sample {i+1}: Predicted: {predicted_label}, Actual: {y_test[i]}")
    </pre>
  </div>

  <div class="program">
    <h2>7. Implement a CNN to Classify Chest X-Ray Images as Pneumonia or Normal</h2>
    <pre>
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

np.random.seed(1)
x_train = np.random.rand(100, 128, 128, 1)
y_train = np.random.randint(0, 2, (100,))
x_test = np.random.rand(20, 128, 128, 1)
y_test = np.random.randint(0, 2, (20,))

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=3, batch_size=16, verbose=1)

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {test_accuracy:.4f}")

predictions = model.predict(x_test[:5])
print("Predictions for first 5 test images:")
for i in range(5):
    predicted_label = 1 if predictions[i] >= 0.5 else 0
    print(f"Sample {i+1}: Predicted: {predicted_label}, Actual: {y_test[i]}")
    </pre>
  </div>

  <div class="program">
    <h2>8. Fine-Tune a Pretrained BERT Model for Sentiment Analysis on a Dataset Like IMDb for Movie Reviews to Classify Them as Positive or Negative</h2>
    <pre>
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch
import numpy as np

reviews = ["I loved this movie it was great", "This film was terrible and boring", "Amazing story and acting", "I hated it so much", "Really good experience"]
labels = [1, 0, 1, 0, 1]

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

inputs = tokenizer(reviews, padding=True, truncation=True, return_tensors="pt", max_length=128)
inputs['labels'] = torch.tensor(labels)

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        return item
    def __len__(self):
        return len(self.encodings['labels'])

dataset = IMDbDataset(inputs)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=2,
    per_device_train_batch_size=2,
    logging_steps=1,
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

trainer.train()

test_reviews = ["This movie is fantastic", "Awful experience"]
test_inputs = tokenizer(test_reviews, padding=True, truncation=True, return_tensors="pt", max_length=128)
outputs = model(**test_inputs)
predictions = torch.argmax(outputs.logits, dim=1)
print("Predictions for test reviews:")
for i, review in enumerate(test_reviews):
    print(f"Review: {review}, Predicted: {'Positive' if predictions[i] == 1 else 'Negative'}")
    </pre>
  </div>

  <div class="program">
    <h2>9. Utilize a Pretrained MobileNet Model to Classify Images from the Kaggle Cats vs Dogs Dataset into 'Cat' or 'Dog'</h2>
    <pre>
import tensorflow as tf
from tensorflow.keras.applications import MobileNet
from tensorflow.keras import layers, models
import numpy as np

np.random.seed(1)
x_train = np.random.rand(100, 224, 224, 3)
y_train = np.random.randint(0, 2, (100,))
x_test = np.random.rand(20, 224, 224, 3)
y_test = np.random.randint(0, 2, (20,))

base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

model = models.Sequential()
model.add(base_model)
model.add(layers.GlobalAveragePooling2D())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=3, batch_size=16, verbose=1)

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {test_accuracy:.4f}")

predictions = model.predict(x_test[:5])
print("Predictions for first 5 test images:")
for i in range(5):
    predicted_label = 'Dog' if predictions[i] >= 0.5 else 'Cat'
    actual_label = 'Dog' if y_test[i] == 1 else 'Cat'
    print(f"Sample {i+1}: Predicted: {predicted_label}, Actual: {actual_label}")
    </pre>
  </div>

  <div class="program">
    <h2>10. Speech Recognition with LSTM Networks</h2>
    <pre>
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

np.random.seed(1)
x_train = np.random.rand(100, 10, 13)
y_train = np.random.randint(0, 2, (100,))
x_test = np.random.rand(20, 10, 13)
y_test = np.random.randint(0, 2, (20,))

model = models.Sequential()
model.add(layers.LSTM(128, input_shape=(10, 13), return_sequences=False))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=3, batch_size=16, verbose=1)

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {test_accuracy:.4f}")

predictions = model.predict(x_test[:5])
print("Predictions for first 5 test samples:")
for i in range(5):
    predicted_label = 1 if predictions[i] >= 0.5 else 0
    print(f"Sample {i+1}: Predicted: {predicted_label}, Actual: {y_test[i]}")
    </pre>
  </div>

  <div class="program">
    <h2>11. Generative Adversarial Network for Image Generation</h2>
    <pre>
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

def build_generator():
    model = models.Sequential()
    model.add(layers.Dense(128, activation='relu', input_dim=100))
    model.add(layers.Dense(784, activation='sigmoid'))
    model.add(layers.Reshape((28, 28, 1)))
    return model

def build_discriminator():
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=(28, 28, 1)))
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

generator = build_generator()
discriminator = build_discriminator()
discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

discriminator.trainable = False
gan_input = layers.Input(shape=(100,))
generated_image = generator(gan_input)
gan_output = discriminator(generated_image)
gan = models.Model(gan_input, gan_output)
gan.compile(optimizer='adam', loss='binary_crossentropy')

np.random.seed(1)
x_train = np.random.rand(1000, 28, 28, 1)

epochs = 5
batch_size = 32

for epoch in range(epochs):
    noise = np.random.normal(0, 1, (batch_size, 100))
    generated_images = generator.predict(noise, verbose=0)
    
    real_images = x_train[np.random.randint(0, x_train.shape[0], batch_size)]
    labels_real = np.ones((batch_size, 1))
    labels_fake = np.zeros((batch_size, 1))
    
    d_loss_real = discriminator.train_on_batch(real_images, labels_real)
    d_loss_fake = discriminator.train_on_batch(generated_images, labels_fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    
    noise = np.random.normal(0, 1, (batch_size, 100))
    labels_gan = np.ones((batch_size, 1))
    g_loss = gan.train_on_batch(noise, labels_gan)
    
    print(f"Epoch {epoch+1}/{epochs} - D Loss: {d_loss[0]:.4f}, D Accuracy: {d_loss[1]:.4f}, G Loss: {g_loss:.4f}")
    </pre>
  </div>

  <div class="program">
    <h2>12. Variational Autoencoder for Generating Images</h2>
    <pre>
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

latent_dim = 2

encoder_inputs = layers.Input(shape=(28, 28, 1))
x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)
x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)
x = layers.Flatten()(x)
x = layers.Dense(16, activation='relu')(x)
z_mean = layers.Dense(latent_dim)(x)
z_log_var = layers.Dense(latent_dim)(x)

def sampling(args):
    z_mean, z_log_var = args
    epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon

z = layers.Lambda(sampling)([z_mean, z_log_var])
encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')

decoder_inputs = layers.Input(shape=(latent_dim,))
x = layers.Dense(7*7*64, activation='relu')(decoder_inputs)
x = layers.Reshape((7, 7, 64))(x)
x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)
x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)
decoder_outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)
decoder = models.Model(decoder_inputs, decoder_outputs, name='decoder')

vae_outputs = decoder(encoder(encoder_inputs)[2])
vae = models.Model(encoder_inputs, vae_outputs, name='vae')

reconstruction_loss = tf.keras.losses.binary_crossentropy(tf.keras.backend.flatten(encoder_inputs), tf.keras.backend.flatten(vae_outputs))
reconstruction_loss *= 28 * 28
kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
kl_loss = tf.reduce_mean(kl_loss, axis=-1)
kl_loss *= -0.5
vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)
vae.compile(optimizer='adam')

np.random.seed(1)
x_train = np.random.rand(1000, 28, 28, 1)

vae.fit(x_train, epochs=5, batch_size=32, verbose=1)
    </pre>
  </div>

</body>
</html>
